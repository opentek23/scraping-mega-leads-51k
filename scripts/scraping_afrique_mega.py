#!/usr/bin/env python3
"""
ğŸŒ SCRAPING AFRIQUE MEGA - VERSION CORRIGÃ‰E
Script simplifiÃ© et fonctionnel pour GitHub Actions
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import json
import os
from datetime import datetime, timezone
import urllib.parse

class AfriqueScrapingMega:
    def __init__(self):
        self.prospects = []
        self.session = requests.Session()
        
        # Headers rÃ©alistes
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        })
        
        # Configuration simplifiÃ©e
        self.target_leads = int(os.environ.get('TARGET_LEADS', '100'))
        self.test_mode = os.environ.get('TEST_MODE', 'true').lower() == 'true'
        self.run_number = os.environ.get('RUN_NUMBER', '000')
        
        print(f"ğŸ¯ Target leads: {self.target_leads}")
        print(f"ğŸ§ª Test mode: {self.test_mode}")
        print(f"ğŸ”¢ Run number: {self.run_number}")
        
        # RequÃªtes de recherche prÃªtes
        self.search_queries = [
            'CEO startup Nigeria site:linkedin.com',
            'founder Kenya business site:linkedin.com',
            'entrepreneur Ghana tech site:linkedin.com',
            'consultant strategy "South Africa" site:linkedin.com',
            'business advisor Nigeria site:linkedin.com',
            'afrobeats artist Nigeria site:instagram.com',
            'musician Ghana site:twitter.com',
            'content creator "South Africa" site:instagram.com',
            'influencer Lagos Nigeria site:instagram.com',
            'tech entrepreneur Kenya site:linkedin.com'
        ]
    
    def scrape_google_search(self, query, max_results=50):
        """Scrape Google avec gestion d'erreur simple"""
        results = []
        
        try:
            print(f"  ğŸ” Recherche: {query[:50]}...")
            
            # Construction URL
            search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&num=10"
            
            # RequÃªte
            response = self.session.get(search_url, timeout=10)
            
            if response.status_code != 200:
                print(f"    âš ï¸ Status {response.status_code}")
                return results
            
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraction liens
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                
                # Nettoyer URL Google
                if href.startswith('/url?q='):
                    href = href.split('/url?q=')[1].split('&')[0]
                    href = urllib.parse.unquote(href)
                
                # Filtrer liens pertinents
                if any(site in href for site in ['linkedin.com/in/', 'instagram.com/', 'twitter.com/']):
                    link_text = link.get_text(strip=True)
                    
                    result = {
                        'url': href,
                        'text': link_text[:200],
                        'query': query,
                        'found_at': datetime.now(timezone.utc).isoformat()
                    }
                    results.append(result)
                    
                    if len(results) >= max_results:
                        break
            
            print(f"    âœ… TrouvÃ© {len(results)} liens")
            
            # DÃ©lai anti-dÃ©tection
            time.sleep(random.uniform(3, 7))
            
        except Exception as e:
            print(f"    âŒ Erreur: {e}")
        
        return results
    
    def determine_platform(self, url):
        """DÃ©termine la plateforme"""
        if 'linkedin.com' in url:
            return 'linkedin'
        elif 'instagram.com' in url:
            return 'instagram'
        elif 'twitter.com' in url:
            return 'twitter'
        else:
            return 'other'
    
    def determine_target_type(self, query, text):
        """DÃ©termine le type de cible"""
        query_lower = query.lower()
        text_lower = text.lower()
        
        if any(word in query_lower for word in ['ceo', 'founder', 'entrepreneur']):
            return 'entrepreneur'
        elif any(word in query_lower for word in ['consultant', 'advisor']):
            return 'consultant'
        elif any(word in query_lower for word in ['musician', 'artist', 'afrobeats']):
            return 'musicien'
        elif any(word in query_lower for word in ['creator', 'influencer']):
            return 'createur'
        else:
            return 'other'
    
    def determine_country(self, query, text):
        """DÃ©termine le pays"""
        query_lower = query.lower()
        text_lower = text.lower()
        
        if any(word in query_lower for word in ['nigeria', 'lagos']):
            return 'nigeria'
        elif 'kenya' in query_lower:
            return 'kenya'
        elif 'ghana' in query_lower:
            return 'ghana'
        elif 'south africa' in query_lower:
            return 'south_africa'
        else:
            return 'africa_other'
    
    def scrape_all_queries(self):
        """Scrape toutes les requÃªtes"""
        print("ğŸŒ DÃ‰MARRAGE SCRAPING AFRIQUE")
        print("=" * 50)
        
        all_prospects = []
        
        # Limiter les requÃªtes en mode test
        queries_to_use = self.search_queries[:3] if self.test_mode else self.search_queries
        
        for i, query in enumerate(queries_to_use):
            print(f"\nğŸ¯ Query {i+1}/{len(queries_to_use)}")
            
            # Scraping
            results = self.scrape_google_search(query, max_results=20 if self.test_mode else 50)
            
            # Traitement des rÃ©sultats
            for result in results:
                prospect = {
                    'url': result['url'],
                    'text': result['text'],
                    'platform': self.determine_platform(result['url']),
                    'target_type': self.determine_target_type(query, result['text']),
                    'country': self.determine_country(query, result['text']),
                    'source_query': query,
                    'found_at': result['found_at'],
                    'region': 'afrique',
                    'run_number': self.run_number
                }
                all_prospects.append(prospect)
            
            # ArrÃªter si objectif atteint
            if len(all_prospects) >= self.target_leads:
                print(f"ğŸ¯ Objectif {self.target_leads} atteint!")
                break
            
            # DÃ©lai entre requÃªtes
            time.sleep(random.uniform(5, 10))
        
        # Suppression des doublons
        unique_prospects = []
        seen_urls = set()
        
        for prospect in all_prospects:
            if prospect['url'] not in seen_urls:
                seen_urls.add(prospect['url'])
                unique_prospects.append(prospect)
        
        removed = len(all_prospects) - len(unique_prospects)
        if removed > 0:
            print(f"ğŸ§¹ SupprimÃ© {removed} doublons")
        
        print(f"\nğŸ‰ SCRAPING TERMINÃ‰!")
        print(f"ğŸ“Š Total prospects uniques: {len(unique_prospects)}")
        
        return unique_prospects
    
    def save_results(self, prospects):
        """Sauvegarde simplifiÃ©e"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CrÃ©er dossier
        os.makedirs('data', exist_ok=True)
        
        # Noms de fichiers
        csv_file = f"data/afrique_prospects_run{self.run_number}_{timestamp}.csv"
        json_file = f"data/afrique_prospects_run{self.run_number}_{timestamp}.json"
        summary_file = f"data/summary_afrique_run{self.run_number}_{timestamp}.json"
        
        try:
            if prospects:
                # DataFrame
                df = pd.DataFrame(prospects)
                
                # CSV
                df.to_csv(csv_file, index=False, encoding='utf-8')
                print(f"ğŸ’¾ CSV sauvÃ©: {csv_file}")
                
                # JSON
                df.to_json(json_file, orient='records', indent=2)
                print(f"ğŸ’¾ JSON sauvÃ©: {json_file}")
                
                # Statistiques
                stats = {
                    'total_prospects': len(prospects),
                    'by_platform': df['platform'].value_counts().to_dict(),
                    'by_target_type': df['target_type'].value_counts().to_dict(),
                    'by_country': df['country'].value_counts().to_dict(),
                    'run_number': self.run_number,
                    'timestamp': timestamp,
                    'test_mode': self.test_mode
                }
                
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ“Š Stats sauvÃ©es: {summary_file}")
                print(f"\nğŸ“ˆ RÃ‰SULTATS:")
                print(f"  - Total: {len(prospects)} prospects")
                print(f"  - Platforms: {stats['by_platform']}")
                print(f"  - Types: {stats['by_target_type']}")
                print(f"  - Pays: {stats['by_country']}")
                
                return True
            else:
                print("âš ï¸ Aucun prospect trouvÃ©")
                return False
                
        except Exception as e:
            print(f"âŒ Erreur sauvegarde: {e}")
            return False

def main():
    """Fonction principale"""
    print("ğŸš€ GITHUB ACTIONS - SCRAPING AFRIQUE MEGA")
    print("=" * 60)
    
    try:
        # Initialiser
        scraper = AfriqueScrapingMega()
        
        # Scraper
        prospects = scraper.scrape_all_queries()
        
        # Sauvegarder
        success = scraper.save_results(prospects)
        
        if success:
            print(f"\nğŸ‰ SUCCÃˆS! {len(prospects)} prospects trouvÃ©s")
        else:
            print(f"\nâš ï¸ Ã‰CHEC ou aucun rÃ©sultat")
        
        return success
        
    except Exception as e:
        print(f"âŒ ERREUR FATALE: {e}")
        return False

if __name__ == "__main__":
    main()
